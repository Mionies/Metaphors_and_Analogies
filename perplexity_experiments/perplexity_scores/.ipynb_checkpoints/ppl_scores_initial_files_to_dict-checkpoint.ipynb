{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "52f546da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import (\n",
    "                          f1_score,\n",
    "                          precision_recall_fscore_support,\n",
    "                          accuracy_score,\n",
    "                          balanced_accuracy_score,\n",
    "                          precision_score,\n",
    "                          recall_score,\n",
    ")\n",
    "\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "56a5fc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_pairs = [\n",
    "                    'Pairs_Cardillo_random_split', \n",
    "                    'Pairs_Jankowiac_random_split', \n",
    "                    'Pairs_Cardillo_lexical_split', 'Pairs_Jankowiac_lexical_split',\n",
    "                    \"Pairs_Cardillo_set\",\"Pairs_Jankowiac_set\"\n",
    "                    ]\n",
    "datasets_quadruples = [\n",
    "                        'Quadruples_Green_lexical_split', 'Quadruples_Green_random_split',\n",
    "                        'Quadruples_Kmiecik_random_split','Quadruples_Kmiecik_lexical_split_on_CD', 'Quadruples_Kmiecik_lexical_split_on_AB' ,\n",
    "                        'Quadruples_SAT_MET_FILTERED_lexical_split','Quadruples_SAT_MET_FILTERED_random_split',\n",
    "                        \"Quadruples_SAT_MET_FILTERED_set\",'Quadruples_Green_set'\n",
    "                        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c4961ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label, label2int = {},{}\n",
    "int2label[\"pair\"] = {\n",
    "  0 : \"anomalous\",\n",
    "  1 : \"literal\",\n",
    "  2 : \"metaphoric\",\n",
    "}\n",
    "int2label[\"quadruple\"] = {\n",
    "  0 : \"anomalous\",\n",
    "  1 : \"analogous\",\n",
    "  2 : \"metaphoric\",\n",
    "}\n",
    "label2int[\"pair\"] = {\n",
    "  \"anomalous\" : 0,\n",
    "  \"literal\" : 1,\n",
    "  \"metaphoric\" : 2,\n",
    "}\n",
    "label2int[\"quadruple\"] = {\n",
    "  \"anomalous\": 0,\n",
    "  \"analogous\": 1,\n",
    "  \"metaphoric\": 2,\n",
    "}\n",
    "mylabels = {\n",
    "    \"Cardillo\":[\n",
    "        \"literal\",\n",
    "        \"metaphoric\"\n",
    "        ],\n",
    "    \"Pairs\": [\n",
    "        \"anomalous\",\n",
    "        \"literal\",\n",
    "        \"metaphoric\"\n",
    "        ],\n",
    "    \"Quadruples\": [\n",
    "        \"anomalous\",\n",
    "        \"analogous\",\n",
    "        \"metaphoric\"\n",
    "        ]\n",
    "}\n",
    "labelset = [ \"anomalous\", \"literal\", \"metaphoric\",\"metaphor\",\"analogous\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "0d7889d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data ={}\n",
    "\n",
    "for data in datasets_pairs:\n",
    "    all_data[data] = load_dataset(\"Joanne/Metaphors_and_Analogies\", data)\n",
    "    \n",
    "    \n",
    "for data in datasets_quadruples:\n",
    "    all_data[data] = load_dataset(\"Joanne/Metaphors_and_Analogies\", data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc5ea984",
   "metadata": {},
   "source": [
    "## All datasets utils dictionaries\n",
    "- datasets_sent2label\n",
    "- datasets_sent2idx\n",
    "- datasets_idx2sent\n",
    "- datasets_datasplits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "358a3f00",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsplits= [\"train\",\"validation\",\"test\"]\n",
    "pairs_dsets = [\"Pairs_Cardillo_set\", \"Pairs_Jankowiac_set\"]\n",
    "quad_dsets=[\"Quadruples_SAT_MET_FILTERED_set\",'Quadruples_Green_set']\n",
    "\n",
    "pairs_cl = [\n",
    "    \"Pairs_Cardillo\",\n",
    "    \"Pairs_Jankowiac\",\n",
    "]\n",
    "\n",
    "quad_cl = [\n",
    "    \"Quadruples_SAT_MET_FILTERED\",\n",
    "    'Quadruples_Green',    \n",
    "]\n",
    "\n",
    "dlab = [\"metaphor\",\"anomaly\",\"literal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "3b441de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_sent2label = {}\n",
    "datasets_sent2idx = {}\n",
    "datasets_idx2sent = {}\n",
    "\n",
    "\n",
    "for dataset in pairs_dsets:\n",
    "    datasets_sent2label[dataset] = {}\n",
    "    datasets_sent2idx[dataset] = {}\n",
    "    datasets_idx2sent[dataset] = {}\n",
    "    n = 0\n",
    "    for i,e in enumerate(all_data[dataset][\"test\"]):\n",
    "        for j,f in enumerate(e[\"sentences\"]):\n",
    "            datasets_sent2label[dataset][f] = e[\"labels\"][j]\n",
    "            datasets_sent2idx[dataset][f] = n\n",
    "            datasets_idx2sent[dataset][n] = f\n",
    "            n+=1\n",
    "            \n",
    "            \n",
    "for dataset in quad_dsets:\n",
    "    datasets_sent2label[dataset] = {}\n",
    "    datasets_sent2idx[dataset] = {}\n",
    "    datasets_idx2sent[dataset] = {}\n",
    "    n = 0\n",
    "    for i,e in enumerate(all_data[dataset][\"test\"]):\n",
    "        for j,f in enumerate(e[\"pairs\"]):\n",
    "            t = eval(e[\"stem\"])\n",
    "            t.extend(f)\n",
    "            t = tuple(t)\n",
    "            datasets_sent2label[dataset][t] = e[\"labels\"][j]\n",
    "            datasets_sent2idx[dataset][t] = n\n",
    "            datasets_idx2sent[dataset][n] = t\n",
    "            n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "b0bd0cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "datasets_datasplits = {}\n",
    "\n",
    "for dataset in pairs_cl:\n",
    "    datasets_datasplits[dataset]={\"lexical\":{},\"random\":{}}\n",
    "    for z in dsplits:\n",
    "        for x in all_data[dataset+\"_random_split\"][z]:\n",
    "            datasets_datasplits[dataset][\"random\"][x[\"sentence\"]]=z  \n",
    "        for x in all_data[dataset+\"_lexical_split\"][z]:\n",
    "            datasets_datasplits[dataset][\"lexical\"][x[\"sentence\"]]=z   \n",
    "            \n",
    "\n",
    "for dataset in quad_cl:\n",
    "    datasets_datasplits[dataset]={\"lexical\":{},\"random\":{}}\n",
    "    for z in dsplits:\n",
    "        for x in all_data[dataset+\"_random_split\"][z]:\n",
    "            t = copy.copy(x[\"AB\"])\n",
    "            t.extend(x[\"CD\"])\n",
    "            t = tuple(t)\n",
    "            datasets_datasplits[dataset][\"random\"][t]=z\n",
    "        for x in all_data[dataset+\"_lexical_split\"][z]:\n",
    "            t = copy.copy(x[\"AB\"])\n",
    "            t.extend(x[\"CD\"])\n",
    "            t = tuple(t)\n",
    "            datasets_datasplits[dataset][\"lexical\"][t]=z  \n",
    "\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "165a469c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def EvaluationMetrics(y_pred,y_true):\n",
    "    analysis={}\n",
    "    #analysis[\"logits\"] = [[y*1 for y in x] for x in logits]\n",
    "    analysis[\"accuracy\"]={}\n",
    "    #micro\n",
    "    analysis[\"micro-f1\"]= round(f1_score(y_true,y_pred,average=\"micro\"),4)\n",
    "    analysis[\"micro-precision\"]=round(precision_score(y_true,y_pred,average=\"micro\"),4)\n",
    "    analysis[\"micro-recall\"]=round(recall_score(y_true,y_pred, average=\"micro\"),4)\n",
    "    # macro\n",
    "    analysis[\"macro-f1\"]=round(f1_score(y_true,y_pred,average=\"macro\"),4)\n",
    "    analysis[\"macro-precision\"]=round(precision_score(y_true,y_pred,average=\"macro\"),4)\n",
    "    analysis[\"macro-recall\"]=round(recall_score(y_true,y_pred,average=\"macro\"),4)\n",
    "    #weighted\n",
    "    analysis[\"weighted-f1\"]=round(f1_score(y_true,y_pred,average=\"weighted\"),4)\n",
    "    analysis[\"weighted-precision\"]=round(precision_score(y_true,y_pred,average=\"weighted\"),4)\n",
    "    analysis[\"weighted-recall\"]=round(recall_score(y_true,y_pred,average=\"weighted\"),4)\n",
    "    #Accuracy\n",
    "    analysis[\"accuracy\"][\"balanced\"]=round(balanced_accuracy_score(y_true,y_pred),4)\n",
    "    analysis[\"accuracy\"][\"standard\"]=round(accuracy_score(y_true,y_pred),4)\n",
    "    analysis[\"class_prf1\"]=[list(x) for x in precision_recall_fscore_support(y_true,y_pred)]\n",
    "    for i,x in enumerate(analysis[\"class_prf1\"][:-1]):\n",
    "        analysis[\"class_prf1\"][i] = [round(y,4) for y in x]\n",
    "    analysis[\"class_prf1\"][-1]= [int(x*1) for x in analysis[\"class_prf1\"][-1 ]]\n",
    "    return analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01098451",
   "metadata": {},
   "source": [
    "# Hsuvas results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3ab3a6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "hsuvas_exp1 = \"hsuvas_results/finetuning-gpt3/\"\n",
    "hsuvas_exp3 = \"hsuvas_results/GPT4-0-shot\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98aed63c",
   "metadata": {},
   "source": [
    "### Finetuning gpt3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f695c8e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadruples_Green_random_split\n",
      "Quadruples_Green_random_split\tgpt3-c\t-\t0.2708\t0.2681\t0.2703\t0.24\t0.2941\n",
      "Quadruples_Green_random_split\tgpt3-b\t-\t0.4167\t0.4216\t0.3529\t0.4118\t0.5\n",
      "Quadruples_Green_random_split\tgpt3-a\t-\t0.3125\t0.3222\t0.1714\t0.4828\t0.3125\n",
      "Quadruples_Green_random_split\tgpt3-d\t-\t0.4167\t0.4323\t0.3226\t0.6154\t0.359\n",
      "Quadruples_SAT_MET_FILTERED_random_split\n",
      "Quadruples_SAT_MET_FILTERED_random_split\tgpt3-a\t-\t0.7398\t0.3073\t0.8499\t0.0719\t0.0\n",
      "Quadruples_SAT_MET_FILTERED_random_split\tgpt3-c\t-\t0.7412\t0.3332\t0.8535\t0.1111\t0.0351\n",
      "Quadruples_SAT_MET_FILTERED_random_split\tgpt3-d\t-\t0.7646\t0.4343\t0.8687\t0.2619\t0.1724\n",
      "Quadruples_SAT_MET_FILTERED_random_split\tgpt3-b\t-\t0.7398\t0.3206\t0.8491\t0.1127\t0.0\n",
      "Pairs_Jankowiac_lexical_split\n",
      "Pairs_Jankowiac_lexical_split\tgpt3-d\t-\t0.8015\t0.803\t0.8081\t0.8675\t0.7333\n",
      "Pairs_Jankowiac_lexical_split\tgpt3-a\t-\t0.3603\t0.1894\t0.0435\t0.0\t0.5246\n",
      "Pairs_Jankowiac_lexical_split\tgpt3-b\t-\t0.6985\t0.6942\t0.7111\t0.6571\t0.7143\n",
      "Pairs_Jankowiac_lexical_split\tgpt3-c\t-\t0.5515\t0.5314\t0.5931\t0.6462\t0.3548\n",
      "Pairs_Jankowiac_random_split\n",
      "Pairs_Jankowiac_random_split\tgpt3-b\t-\t0.6181\t0.6058\t0.6579\t0.507\t0.6525\n",
      "Pairs_Jankowiac_random_split\tgpt3-c\t-\t0.6944\t0.6437\t0.3704\t0.8454\t0.7153\n",
      "Pairs_Jankowiac_random_split\tgpt3-d\t-\t0.5486\t0.495\t0.5941\t0.2593\t0.6316\n",
      "Pairs_Jankowiac_random_split\tgpt3-a\t-\t0.7083\t0.7011\t0.6389\t0.7356\t0.7287\n",
      "Quadruples_Kmiecik_lexical_split_on_AB\n",
      "Quadruples_Kmiecik_lexical_split_on_AB\tgpt3-d\t-\t0.7361\t0.7178\t0.7697\t0.8056\t0.5781\n",
      "Quadruples_Kmiecik_lexical_split_on_AB\tgpt3-a\t-\t0.5208\t0.4421\t0.6584\t0.5333\t0.1345\n",
      "Quadruples_Kmiecik_lexical_split_on_AB\tgpt3-b\t-\t0.5417\t0.5306\t0.577\t0.5873\t0.4276\n",
      "Quadruples_Kmiecik_lexical_split_on_AB\tgpt3-c\t-\t0.6319\t0.6085\t0.6735\t0.7179\t0.4341\n",
      "Quadruples_Kmiecik_lexical_split_on_CD\n",
      "Quadruples_Kmiecik_lexical_split_on_CD\tgpt3-c\t-\t0.5417\t0.4899\t0.6039\t0.6497\t0.2162\n",
      "Quadruples_Kmiecik_lexical_split_on_CD\tgpt3-a\t-\t0.5694\t0.5094\t0.635\t0.6331\t0.26\n",
      "Quadruples_Kmiecik_lexical_split_on_CD\tgpt3-d\t-\t0.559\t0.5007\t0.6006\t0.6747\t0.2268\n",
      "Quadruples_Kmiecik_lexical_split_on_CD\tgpt3-b\t-\t0.625\t0.6164\t0.6341\t0.7226\t0.4925\n",
      "Pairs_Cardillo_lexical_split\n",
      "Quadruples_Kmiecik_random_split\n",
      "Quadruples_Kmiecik_random_split\tgpt3-d\t-\t0.7917\t0.7767\t0.8179\t0.7867\t0.7257\n",
      "Quadruples_Kmiecik_random_split\tgpt3-c\t-\t0.5694\t0.5534\t0.5914\t0.7123\t0.3566\n",
      "Quadruples_Kmiecik_random_split\tgpt3-b\t-\t0.5347\t0.4984\t0.5926\t0.5778\t0.3248\n",
      "Quadruples_Kmiecik_random_split\tgpt3-a\t-\t0.4132\t0.2247\t0.5852\t0.0889\t0.0\n",
      "Pairs_Cardillo_random_split\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\tgpt3-d\t-\t0.8423\t0.5844\t0.9143\t0.5\t0.339\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\tgpt3-a\t-\t0.8\t0.2963\t0.8889\t0.0\t0.0\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\tgpt3-c\t-\t0.7869\t0.3862\t0.8829\t0.2016\t0.0741\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\tgpt3-b\t-\t0.7766\t0.3339\t0.8726\t0.129\t0.0\n",
      "Quadruples_Green_lexical_split\n",
      "Quadruples_Green_lexical_split\tgpt3-c\t-\t0.3958\t0.3056\t0.2727\t0.5263\t0.1176\n",
      "Quadruples_Green_lexical_split\tgpt3-a\t-\t0.3958\t0.3655\t0.4615\t0.4444\t0.1905\n",
      "Quadruples_Green_lexical_split\tgpt3-b\t-\t0.5\t0.5044\t0.4762\t0.5926\t0.4444\n",
      "Quadruples_Green_lexical_split\tgpt3-d\t-\t0.4583\t0.3662\t0.5532\t0.5455\t0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/joanne/.local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "for data_set in os.listdir(hsuvas_exp1):\n",
    "    res_file = os.listdir(hsuvas_exp1+\"/\"+data_set)\n",
    "    for re in res_file:\n",
    "        p = hsuvas_exp1+\"/\"+data_set+\"/\"+re\n",
    "        r = open(p).readlines()\n",
    "        r = [x.strip().split(\",\") for x in r ]\n",
    "        k = {}\n",
    "        k[\"idx\"]=[x[0] for x in r[1:]]\n",
    "        k[\"text\"]=[x[1] for x in r[1:]]\n",
    "        k[\"true\"]=[int(x[2]) for x in r[1:]]\n",
    "        k[\"predicted\"]=[int(x[3]) for x in r[1:]]\n",
    "        try:\n",
    "            k[\"logits\"]=[[float(x[4]),float(x[5]),float(x[6])] for x in r[1:]]\n",
    "        except:\n",
    "            k[\"logits\"]=[[float(x[4]),float(x[5])] for x in r[1:]]\n",
    "        k[\"model\"]=\"gpt3-\"+re[-5]\n",
    "        k[\"dataset\"]=re[:-6]\n",
    "        if not k[\"dataset\"] in results:\n",
    "            results[k[\"dataset\"]]={}\n",
    "        results[k[\"dataset\"]][k[\"model\"]]=k\n",
    " \n",
    "        \n",
    "for x in results:\n",
    "    for y in results[x]:\n",
    "        #print(len(results[x][y][\"predicted\"]),len(results[x][y][\"true\"]))\n",
    "        results[x][y][\"class_metrics\"]=EvaluationMetrics(results[x][y][\"predicted\"],results[x][y][\"true\"])\n",
    "        \n",
    "head = [\"Dataset\",\"model\",\"size\", \"Acc.\",\"Macro-F1\",\"Anomaly F1\",\"Literal F1\",\"Metaphor F1\"]  \n",
    "lines = []\n",
    "for x in results:\n",
    "    print(x)\n",
    "    if \"Cardillo\" in x:\n",
    "        for y in results[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                y,\n",
    "                \"-\",\n",
    "                results[x][y][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results[x][y][\"class_metrics\"][\"macro-f1\"],\n",
    "                \"-\",\n",
    "                results[x][y][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results[x][y][\"class_metrics\"][\"class_prf1\"][2][1]\n",
    "            ]\n",
    "            lines.append(line)\n",
    "    else:\n",
    "        for y in results[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                y,\n",
    "                \"-\",\n",
    "                results[x][y][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results[x][y][\"class_metrics\"][\"macro-f1\"],\n",
    "                results[x][y][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results[x][y][\"class_metrics\"][\"class_prf1\"][2][1],\n",
    "                results[x][y][\"class_metrics\"][\"class_prf1\"][2][2],\n",
    "            ]\n",
    "            print(\"\\t\".join([str(x) for x in line]))\n",
    "            lines.append(line)\n",
    "            \n",
    "            \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_finetuning_gpt3.json\", \"w\") as outfile:\n",
    "    json.dump(results,outfile,indent=\"\\t\")\n",
    "    \n",
    "filename = \"GPT3_finetuning.csv\"\n",
    "df = pd.DataFrame(lines,columns=head)\n",
    "df.to_csv(filename, index=False)         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ea47e7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"hf_index\",\"prediction\",\"true\",\"item\"]\n",
    "for x in results:\n",
    "    for y in results[x]:\n",
    "        lines = []\n",
    "        filename = \"error_analysis/\"+y+\"_finetuning_\"+x+\".csv\"\n",
    "        for i,e in enumerate(results[x][y][\"true\"]): \n",
    "            if results[x][y][\"true\"][i]!=results[x][y][\"predicted\"][i]:\n",
    "                line = [\n",
    "                    results[x][y][\"idx\"][i],\n",
    "                    results[x][y][\"predicted\"][i],\n",
    "                    results[x][y][\"true\"][i],\n",
    "                    results[x][y][\"text\"][i]\n",
    "                ]\n",
    "                lines.append(line)\n",
    "        df = pd.DataFrame(lines,columns=head)\n",
    "        df.to_csv(filename, index=False)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7986073c",
   "metadata": {},
   "source": [
    "## Zero shot GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "099e1945",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy    \n",
    "dsimp = {\n",
    "    'Quadruples_SAT_MET_FILTERED_random_split':'Quadruples_SAT_MET_FILTERED_set',\n",
    "    'Quadruples_SAT_MET_FILTERED_lexical_split':'Quadruples_SAT_MET_FILTERED_set',\n",
    "    'Quadruples_Green_lexical_split':'Quadruples_Green_set',\n",
    "    'Quadruples_Green_random_split':'Quadruples_Green_set',\n",
    "    \"Pairs_Jankowiac_random_split\":\"Pairs_Jankowiac_set\",\n",
    "    \"Pairs_Jankowiac_lexical_split\":\"Pairs_Jankowiac_set\",\n",
    "    'Pairs_Cardillo_lexical_split':'Pairs_Cardillo_set',\n",
    "    'Pairs_Cardillo_random_split':'Pairs_Cardillo_set',\n",
    "    'Pairs_Cardillo_lexical_split_test':'Pairs_Cardillo_set',\n",
    "    'Pairs_Cardillo_random_split_test':'Pairs_Cardillo_set',\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "17cbd04d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pairs_Jankowiac_random_split_gpt4_responses.json\n",
      "Pairs_Jankowiac_lexical_split_gpt4_responses.json\n",
      "Pairs_Cardillo_lexical_split_gpt4_responses.json\n",
      "Quadruples_SAT_MET_FILTERED_random_split_gpt4_responses.json\n",
      "Quadruples_Green_lexical_split_gpt4_responses.json\n",
      "Pairs_Cardillo_random_split_test_gpt4_responses.json\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split_gpt4_responses.json\n",
      "Pairs_Cardillo_lexical_split_test_gpt4_responses.json\n",
      "Pairs_Cardillo_random_split_gpt4_responses.json\n",
      "Quadruples_Green_random_split_gpt4_responses.json\n",
      "Pairs_Jankowiac_random_split\n",
      "Pairs_Jankowiac_lexical_split\n",
      "Pairs_Cardillo_lexical_split\n",
      "Quadruples_SAT_MET_FILTERED_random_split\n",
      "Quadruples_Green_lexical_split\n",
      "Pairs_Cardillo_random_split_test\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\n",
      "Pairs_Cardillo_lexical_split_test\n",
      "Pairs_Cardillo_random_split\n",
      "Quadruples_Green_random_split\n",
      "Pairs_Cardillo_lexical_split\n",
      "\t\t\t\t\t 0.8565 [0.8381, 0.8748]\n",
      "Pairs_Cardillo_lexical_split_test\n",
      "\t\t\t\t\t 0.8376 [0.8156, 0.8596]\n",
      "Pairs_Cardillo_random_split\n",
      "\t\t\t\t\t 0.8669 [0.8522, 0.8815]\n",
      "Pairs_Cardillo_random_split_test\n",
      "\t\t\t\t\t 0.8647 [0.8627, 0.8667]\n",
      "Pairs_Jankowiac_lexical_split\n",
      "\t\t\t\t\t 0.9712 [0.9535, 1.0, 0.96]\n",
      "Pairs_Jankowiac_random_split\n",
      "\t\t\t\t\t 0.9795 [0.9762, 0.9892, 0.973]\n",
      "Quadruples_Green_lexical_split\n",
      "\t\t\t\t\t 0.5854 [0.64, 0.5778, 0.5385]\n",
      "Quadruples_Green_random_split\n",
      "\t\t\t\t\t 0.4633 [0.5385, 0.4348, 0.4167]\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\n",
      "\t\t\t\t\t 0.4199 [0.5118, 0.3417, 0.4062]\n",
      "Quadruples_SAT_MET_FILTERED_random_split\n",
      "\t\t\t\t\t 0.4279 [0.5249, 0.3379, 0.4211]\n",
      "Pairs_Jankowiac_random_split\n",
      "Pairs_Jankowiac_random_split\tGPT4\t-\t0.9792\t0.9795\t0.9762\t0.9892\t0.973\n",
      "Pairs_Jankowiac_lexical_split\n",
      "Pairs_Jankowiac_lexical_split\tGPT4\t-\t0.9706\t0.9712\t0.9535\t1.0\t0.96\n",
      "Pairs_Cardillo_lexical_split\n",
      "Quadruples_SAT_MET_FILTERED_random_split\n",
      "Quadruples_SAT_MET_FILTERED_random_split\tGPT4\t-\t0.4451\t0.4279\t0.5249\t0.3379\t0.4211\n",
      "Quadruples_Green_lexical_split\n",
      "Quadruples_Green_lexical_split\tGPT4\t-\t0.5833\t0.5854\t0.64\t0.5778\t0.5385\n",
      "Pairs_Cardillo_random_split_test\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\n",
      "Quadruples_SAT_MET_FILTERED_lexical_split\tGPT4\t-\t0.4371\t0.4199\t0.5118\t0.3417\t0.4062\n",
      "Pairs_Cardillo_lexical_split_test\n",
      "Pairs_Cardillo_random_split\n",
      "Quadruples_Green_random_split\n",
      "Quadruples_Green_random_split\tGPT4\t-\t0.4583\t0.4633\t0.5385\t0.4348\t0.4167\n"
     ]
    }
   ],
   "source": [
    "results_e3 = {}\n",
    "for res_file in os.listdir(hsuvas_exp3):\n",
    "    print(res_file)\n",
    "    d = res_file[:-20]\n",
    "    results_e3[d]={\"true\":[],\"predicted\":[],\"model\":res_file[21:25],\"dataset\":d,\"experiment\":\"zero-shot-instruction\"}\n",
    "    if \"SAT\" in d or \"Green\" in d:\n",
    "        results_e3[d][\"pair1\"]=[]\n",
    "        results_e3[d][\"pair2\"]=[]\n",
    "        results_e3[d][\"explanation\"]=[]\n",
    "    elif \"Cardillo\" in d:\n",
    "        results_e3[d][\"text\"]=[]\n",
    "        results_e3[d][\"pair\"]=[]\n",
    "        results_e3[d][\"explanation\"]=[]           \n",
    "    else:\n",
    "        results_e3[d][\"text\"]=[]    \n",
    "        #results_e3[d][\"true\"]=([e[\"label\"] for e in all_data[d][\"test\"]])\n",
    "    p = hsuvas_exp3+\"/\"+res_file\n",
    "    r = open(p)\n",
    "    for i,line in enumerate(r):\n",
    "        try:\n",
    "            a = eval(eval(line))\n",
    "        except:\n",
    "            a = json.loads(line)\n",
    "        if \"SAT\" in d or \"Green\" in d:\n",
    "            try:\n",
    "                pr,p1,p2,ex = a[\"label\"],a[\"pair1\"],a[\"pair2\"],a[\"explanation\"]\n",
    "            except:\n",
    "                print(\"k\",a)\n",
    "                pr,p1,p2,ex = a[\"label\"],a[\"pair\"][0],a[\"pair\"][1],a[\"explanation\"]\n",
    "            results_e3[d][\"predicted\"].append(pr)\n",
    "            results_e3[d][\"pair1\"].append(p1)\n",
    "            results_e3[d][\"pair2\"].append(p2)\n",
    "            results_e3[d][\"explanation\"].append(ex)         \n",
    "        elif \"Cardillo\" in d:\n",
    "            try:\n",
    "            #if 1:\n",
    "                results_e3[d][\"predicted\"].append(a[\"labels\"])\n",
    "                results_e3[d][\"text\"].append(a[\"sentence\"])\n",
    "                results_e3[d][\"explanation\"].append(a[\"explanation\"]) \n",
    "                results_e3[d][\"pair\"].append(a[\"pair\"]) \n",
    "            except:\n",
    "                print(a)\n",
    "        else:\n",
    "            results_e3[d][\"predicted\"].append(a[\"output\"])\n",
    "            results_e3[d][\"text\"].append(a[\"input\"])\n",
    "\n",
    "    \n",
    "for d in results_e3:\n",
    "    k = dsimp[d]\n",
    "    if \"Quadruple\" in d:\n",
    "        for i in range(len(results_e3[d][\"pair1\"])): \n",
    "            insti = copy.copy(results_e3[d][\"pair1\"][i])\n",
    "            insti.extend(results_e3[d][\"pair2\"][i])\n",
    "            insti = tuple(insti)\n",
    "            results_e3[d][\"true\"].append(datasets_sent2label[k][insti])\n",
    "    elif \"Pair\" in d:\n",
    "        for i in range(len(results_e3[d][\"text\"])): \n",
    "            insti =results_e3[d][\"text\"][i]\n",
    "            #print(k,insti)\n",
    "            try:\n",
    "                results_e3[d][\"true\"].append(datasets_sent2label[k][insti])\n",
    "            except:\n",
    "                try:\n",
    "                    results_e3[d][\"true\"].append(datasets_sent2label[k][insti+\" \"])\n",
    "                except:\n",
    "                    insti = insti[:-1]+\" .\"\n",
    "                    results_e3[d][\"true\"].append(datasets_sent2label[k][insti])\n",
    "            \n",
    "for x in results_e3:\n",
    "    print(x)\n",
    "    results_e3[x][\"class_metrics\"]=EvaluationMetrics(results_e3[x][\"predicted\"],results_e3[x][\"true\"])\n",
    "    \n",
    "o = list(results_e3.keys())\n",
    "o.sort()\n",
    "for x in o:\n",
    "    print(x)\n",
    "    print(\"\\t\\t\\t\\t\\t\", results_e3[x][\"class_metrics\"][\"macro-f1\"],results_e3[x][\"class_metrics\"][\"class_prf1\"][2])\n",
    "    \n",
    "\n",
    "head = [\"Dataset\",\"model\",\"size\", \"Acc.\",\"Macro-F1\",\"Anomaly F1\",\"Literal F1\",\"Metaphor F1\"]  \n",
    "lines = []\n",
    "for x in results_e3:\n",
    "    print(x)\n",
    "    if \"Cardillo\" in x:\n",
    "        #for y in results_e3[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                \"GPT4\",\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results_e3[x][\"class_metrics\"][\"macro-f1\"],\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][1]\n",
    "            ]\n",
    "            lines.append(line)\n",
    "    else:\n",
    "        #for y in results_e3[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                \"GPT4\",\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results_e3[x][\"class_metrics\"][\"macro-f1\"],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][1],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][2],\n",
    "            ]\n",
    "            print(\"\\t\".join([str(x) for x in line]))\n",
    "            lines.append(line)\n",
    "        \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_zeroshot_1by1_gpt4.json\", \"w\") as outfile:\n",
    "    json.dump(results_e3,outfile,indent=\"\\t\")\n",
    "    \n",
    "filename = \"GPT4_zeroshot_1by1.csv\"\n",
    "df = pd.DataFrame(lines,columns=head)\n",
    "df.to_csv(filename, index=False)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "999d6760",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = hsuvas_exp3+\"/\"+res_file\n",
    "r = open(p)\n",
    "for i,line in enumerate(r):\n",
    "    #try:\n",
    "    #a = eval(eval(line))\n",
    "    a = eval(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "80d75a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "head = [\"hf_index\",\"prediction\",\"true\",\"item\"]\n",
    "for x in results_e3:\n",
    "    #for y in results_e3[x]:\n",
    "    lines = []\n",
    "    filename = \"error_analysis/GPT4_zeroshot_1by1_\"+x+\".csv\"\n",
    "    for i,e in enumerate(results_e3[x][\"true\"]): \n",
    "        if results_e3[x][\"true\"][i]!=results_e3[x][\"predicted\"][i]:\n",
    "            try:\n",
    "                line = [\n",
    "                    i,\n",
    "                    results_e3[x][\"predicted\"][i],\n",
    "                    results_e3[x][\"true\"][i],\n",
    "                    results_e3[x][\"text\"][i]\n",
    "                ]\n",
    "            except:\n",
    "                c = copy.copy(results_e3[x][\"pair1\"][i])\n",
    "                c.extend(results_e3[x][\"pair2\"][i])\n",
    "                line = [\n",
    "                    i,\n",
    "                    results_e3[x][\"predicted\"][i],\n",
    "                    results_e3[x][\"true\"][i],\n",
    "                    c\n",
    "                ]\n",
    "            lines.append(line)\n",
    "    df = pd.DataFrame(lines,columns=head)\n",
    "    df.to_csv(filename, index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f20feb15",
   "metadata": {},
   "source": [
    "## Zero shot GPT 3.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b760bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quadruples_Green_random_split\n",
      "Quadruples_Green_random_split_gpt3_responses.json\n",
      "47\n",
      "Pairs_Jankowiac_lexical_split\n",
      "Pairs_Jankowiac_lexical_split_gpt3_responses.json\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Invalid key: 136 is out of bounds for size 136",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [25]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m     19\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28meval\u001b[39m(line)\n\u001b[0;32m---> 20\u001b[0m results_e3[d][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(\u001b[43mall_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43md\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSAT\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m d \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGreen\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m d \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKm\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m d:\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2227\u001b[0m, in \u001b[0;36mDataset.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):  \u001b[38;5;66;03m# noqa: F811\u001b[39;00m\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;124;03m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[39;00m\n\u001b[0;32m-> 2227\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/arrow_dataset.py:2211\u001b[0m, in \u001b[0;36mDataset._getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2209\u001b[0m format_kwargs \u001b[38;5;241m=\u001b[39m format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m   2210\u001b[0m formatter \u001b[38;5;241m=\u001b[39m get_formatter(format_type, features\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures, decoded\u001b[38;5;241m=\u001b[39mdecoded, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mformat_kwargs)\n\u001b[0;32m-> 2211\u001b[0m pa_subtable \u001b[38;5;241m=\u001b[39m \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   2212\u001b[0m formatted_output \u001b[38;5;241m=\u001b[39m format_table(\n\u001b[1;32m   2213\u001b[0m     pa_subtable, key, formatter\u001b[38;5;241m=\u001b[39mformatter, format_columns\u001b[38;5;241m=\u001b[39mformat_columns, output_all_columns\u001b[38;5;241m=\u001b[39moutput_all_columns\n\u001b[1;32m   2214\u001b[0m )\n\u001b[1;32m   2215\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/formatting/formatting.py:486\u001b[0m, in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    485\u001b[0m     size \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mnum_rows \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m table\u001b[38;5;241m.\u001b[39mnum_rows\n\u001b[0;32m--> 486\u001b[0m     \u001b[43m_check_valid_index_key\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/datasets/formatting/formatting.py:429\u001b[0m, in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m key \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 429\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid key: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is out of bounds for size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[0;31mIndexError\u001b[0m: Invalid key: 136 is out of bounds for size 136"
     ]
    }
   ],
   "source": [
    "results_e3 = {}\n",
    "for res_file in os.listdir(\"hsuvas_results/GPT3.5-0shot\"):\n",
    "    d = res_file[:-20]\n",
    "    print(d)\n",
    "    results_e3[d]={\"true\":[],\"predicted\":[],\"model\":res_file[21:25],\"dataset\":d,\"experiment\":\"zero-shot-instruction\"}\n",
    "    if \"SAT\" in d or \"Green\" in d or \"Km\" in d:\n",
    "        results_e3[d][\"pair1\"]=[]\n",
    "        results_e3[d][\"pair2\"]=[]\n",
    "        results_e3[d][\"explanation\"]=[]\n",
    "    else:\n",
    "        results_e3[d][\"text\"]=[]\n",
    "    print(res_file)\n",
    "    p = \"hsuvas_results/GPT3.5-0shot/\"+res_file\n",
    "    r = open(p)\n",
    "    for i,line in enumerate(r):\n",
    "        try:\n",
    "            a = eval(eval(line))\n",
    "        except:\n",
    "            a = eval(line)\n",
    "        results_e3[d][\"true\"].append(all_data[d][\"test\"][i][\"label\"])\n",
    "        if \"SAT\" in d or \"Green\" in d or \"Km\" in d:\n",
    "            try:\n",
    "                pr,p1,p2,ex = a[\"label\"],a[\"pair1\"],a[\"pair2\"],a[\"explanation\"]\n",
    "            except:\n",
    "                try:\n",
    "                    pr,p1,p2,ex = a[\"label\"],a[\"pair\"][0],a[\"pair\"][1],a[\"explanation\"]\n",
    "                except:\n",
    "                    try:\n",
    "                        pr,p1,p2,ex = a[\"labels\"],a[\"pair\"][0],a[\"pair\"][1],a[\"explanation\"]\n",
    "                    except:\n",
    "                        print(a)\n",
    "            results_e3[d][\"predicted\"].append(pr)\n",
    "            results_e3[d][\"pair1\"].append(p1)\n",
    "            results_e3[d][\"pair2\"].append(p2)\n",
    "            results_e3[d][\"explanation\"].append(ex) \n",
    "        elif \"Cardillo\" in d or \"Jank\" in d:\n",
    "            try:\n",
    "                results_e3[d][\"predicted\"].append(a[\"label\"])\n",
    "                results_e3[d][\"text\"].append([[x[0].lower(),x[1].lower()] for x in a[\"pair\"]])\n",
    "            except:\n",
    "                #try:\n",
    "                if 1:\n",
    "                    results_e3[d][\"predicted\"].append(a[\"labels\"])\n",
    "                    results_e3[d][\"text\"].append([[x[0].lower(),x[1].lower()] for x in a[\"pair\"]])\n",
    "                #except:\n",
    "                    #print(\"fk\",a)\n",
    "        else:\n",
    "            try:\n",
    "                results_e3[d][\"predicted\"].append(a[\"output\"])\n",
    "                results_e3[d][\"text\"].append(a[\"input\"])\n",
    "            except:\n",
    "                print(a)\n",
    "    print(i)\n",
    "        \n",
    "    #all_data['Pairs_Jankowiac_lexical_split'][\"tes\n",
    "    \n",
    "    \n",
    "for x in results_e3:\n",
    "    results_e3[x][\"class_metrics\"]=EvaluationMetrics(results_e3[x][\"predicted\"],results_e3[x][\"true\"])\n",
    "\n",
    "    \n",
    "for x in results_e3:\n",
    "    print(x)\n",
    "    print(\"\\t\\t\\t\\t\\t\", results_e3[x][\"class_metrics\"][\"macro-f1\"],results_e3[x][\"class_metrics\"][\"class_prf1\"][2])   \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "head = [\"Dataset\",\"model\",\"size\", \"Acc.\",\"Macro-F1\",\"Anomaly F1\",\"Literal F1\",\"Metaphor F1\"]  \n",
    "lines = []\n",
    "for x in results_e3:\n",
    "    print(x)\n",
    "    if \"Cardillo\" in x:\n",
    "        #for y in results_e3[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                \"GPT3.5\",\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results_e3[x][\"class_metrics\"][\"macro-f1\"],\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][1]\n",
    "            ]\n",
    "            lines.append(line)\n",
    "    else:\n",
    "        #for y in results_e3[x]:\n",
    "            line = [\n",
    "                x,\n",
    "                \"GPT3.5\",\n",
    "                \"-\",\n",
    "                results_e3[x][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "                results_e3[x][\"class_metrics\"][\"macro-f1\"],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][0],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][1],\n",
    "                results_e3[x][\"class_metrics\"][\"class_prf1\"][2][2],\n",
    "            ]\n",
    "            print(\"\\t\".join([str(x) for x in line]))\n",
    "            lines.append(line)\n",
    "        \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_zeroshot_1by1_gpt3.5.json\", \"w\") as outfile:\n",
    "    json.dump(results_e3,outfile,indent=\"\\t\")\n",
    "    \n",
    "filename = \"GPT3.5_zeroshot_1by1.csv\"\n",
    "df = pd.DataFrame(lines,columns=head)\n",
    "df.to_csv(filename, index=False)     \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "59b66673",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror_analysis/GPT3.5_zeroshot_1by1_\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m+\u001b[39mx\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i,e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(results_e3[x][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m]): \n\u001b[0;32m----> 8\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m results_e3[x][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\u001b[38;5;241m!=\u001b[39m\u001b[43mresults_e3\u001b[49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpredicted\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m:\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m             line \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     11\u001b[0m                 i,\n\u001b[1;32m     12\u001b[0m                 results_e3[x][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m     13\u001b[0m                 results_e3[x][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m][i],\n\u001b[1;32m     14\u001b[0m                 results_e3[x][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m][i]\n\u001b[1;32m     15\u001b[0m             ]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "head = [\"hf_index\",\"prediction\",\"true\",\"item\"]\n",
    "for x in results_e3:\n",
    "    #for y in results_e3[x]:\n",
    "    lines = []\n",
    "    filename = \"error_analysis/GPT3.5_zeroshot_1by1_\"+x+\".csv\"\n",
    "    for i,e in enumerate(results_e3[x][\"true\"]): \n",
    "        if results_e3[x][\"true\"][i]!=results_e3[x][\"predicted\"][i]:\n",
    "            try:\n",
    "                line = [\n",
    "                    i,\n",
    "                    results_e3[x][\"predicted\"][i],\n",
    "                    results_e3[x][\"true\"][i],\n",
    "                    results_e3[x][\"text\"][i]\n",
    "                ]\n",
    "            except:\n",
    "                c = copy.copy(results_e3[x][\"pair1\"][i])\n",
    "                c.extend(results_e3[x][\"pair2\"][i])\n",
    "                line = [\n",
    "                    i,\n",
    "                    results_e3[x][\"predicted\"][i],\n",
    "                    results_e3[x][\"true\"][i],\n",
    "                    c\n",
    "                ]\n",
    "            lines.append(line)\n",
    "    df = pd.DataFrame(lines,columns=head)\n",
    "    df.to_csv(filename, index=False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9da03fc",
   "metadata": {},
   "source": [
    "# Lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d2f6065f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dsplits= [\"train\",\"validation\",\"test\"]\n",
    "pairs_dsets = [\"Pairs_Cardillo_set\", \"Pairs_Jankowiac_set\"]\n",
    "quad_dsets=[\"Quadruples_SAT_MET_FILTERED_set\",'Quadruples_Green_set']\n",
    "\n",
    "pairs_cl = [\n",
    "    \"Pairs_Cardillo\",\n",
    "    \"Pairs_Jankowiac\",\n",
    "]\n",
    "\n",
    "quad_cl = [\n",
    "    \"Quadruples_SAT_MET_FILTERED\",\n",
    "    'Quadruples_Green',    \n",
    "]\n",
    "\n",
    "dlab = [\"metaphor\",\"anomaly\",\"literal\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e332793",
   "metadata": {},
   "source": [
    "# Asahi results\n",
    "## All models perplexities\n",
    "### Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95414483",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t5-11b.Metaphors_and_Analogies_Quadruples_Green_set_test.json', 'roberta-base.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'gpt-neo-2.7B.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 'opt-66b.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json'] \n",
      "\n",
      " ['babbage.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 'davinci.Metaphors_and_Analogies_Quadruples_Green_set_test.json']\n"
     ]
    }
   ],
   "source": [
    "# Score : is X , SAT_MET\n",
    "asahi_sat_score = 'asahi_results/metaphor_results/scores_sat/'\n",
    "s = os.listdir(asahi_sat_score)\n",
    "asahi_sat_score_met = []\n",
    "asahi_sat_score_lit = []\n",
    "asahi_sat_score_ano = []\n",
    "\n",
    "for y in s:\n",
    "    x = y.split(\".\")\n",
    "    model,t = x[0],x[1]\n",
    "    if t==\"metaphor\":\n",
    "        asahi_sat_score_met.append([y,model])\n",
    "    elif t==\"literal\":\n",
    "        asahi_sat_score_lit.append([y,model])\n",
    "    elif t == \"anomaly\":\n",
    "        asahi_sat_score_ano.append([y,model])\n",
    "\n",
    "        \n",
    "# Score : is X , other sets\n",
    "asahi_scores = \"asahi_results/metaphor_results/scores/\"\n",
    "asahi_scores_openai = \"asahi_results/metaphor_results/scores_openai/\"    \n",
    "        \n",
    "\n",
    "scores = os.listdir(asahi_scores)\n",
    "scores_openai = os.listdir(asahi_scores_openai)\n",
    "\n",
    "print(scores[:4],'\\n\\n',scores_openai[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4f133b",
   "metadata": {},
   "source": [
    "### No prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d805cbe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t5-11b.Metaphors_and_Analogies_Quadruples_Green_set_test.json', 'roberta-base.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'gpt-neo-2.7B.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 'opt-66b.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json'] \n",
      "\n",
      " ['babbage.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 'davinci.Metaphors_and_Analogies_Quadruples_Green_set_test.json']\n"
     ]
    }
   ],
   "source": [
    "asahi_no_prompt = \"asahi_results/metaphor_results/scores_no_prompt/\"\n",
    "asahi_no_prompt_openai = \"asahi_results/metaphor_results/scores_openai_no_prompt/\"   \n",
    "\n",
    "# SAT\n",
    "\n",
    "\n",
    "# Open Models\n",
    "# Open AI\n",
    "\n",
    "no_prompt = os.listdir(asahi_no_prompt)\n",
    "no_prompt_openai = os.listdir(asahi_no_prompt_openai)\n",
    "\n",
    "print(no_prompt[:4],'\\n\\n',no_prompt_openai[:4])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eedbfca",
   "metadata": {},
   "source": [
    "### Instructions scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "f84c80af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['t5-11b.Metaphors_and_Analogies_Quadruples_Green_set_test.json', 'roberta-base.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'opt-66b.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 't5-large.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json'] \n",
      "\n",
      " ['babbage.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Cardillo_set_test.json', 'curie.Metaphors_and_Analogies_Pairs_Jankowiac_set_test.json', 'davinci.Metaphors_and_Analogies_Quadruples_Green_set_test.json']\n"
     ]
    }
   ],
   "source": [
    "asahi_instructions = \"asahi_results/metaphor_results/scores_instruction/\"\n",
    "asahi_openai_instructions = \"asahi_results/metaphor_results/scores_openai_instruction/\"   \n",
    "\n",
    "# Open Models\n",
    "# OpenAI\n",
    "\n",
    "instructions = os.listdir(asahi_instructions)\n",
    "instructions_openai = os.listdir(asahi_openai_instructions)\n",
    "\n",
    "print(instructions[:4],'\\n\\n',instructions_openai[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "417813df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Flatten(ll):\n",
    "    l =[]\n",
    "    for x in ll:\n",
    "        for y in x:\n",
    "            l.append(y)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f277e0e",
   "metadata": {},
   "source": [
    "### Score dictionary - all datasets but SAT_MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "37d202bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open source\n",
    "\n",
    "results_score ={} \n",
    "\n",
    "for x in scores:#[:1]:\n",
    "    p = json.load(open(asahi_scores+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    \n",
    "    if not dataset in results_score:\n",
    "        results_score[dataset]={\"metaphor\":{},\"anomaly\":{},\"literal\":{}}\n",
    "    for z in dlab:\n",
    "        if not model in results_score[dataset][z]:    \n",
    "            results_score[dataset][z][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "        for i,y in enumerate(p[z]):\n",
    "            results_score[dataset][z][model][\"score\"].append(y[\"score\"])\n",
    "            results_score[dataset][z][model][\"idx\"].append(y[\"index\"])\n",
    "            results_score[dataset][z][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "            assert results_score[dataset][z][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]\n",
    "\n",
    "#    results_score[\"metaphor\"][x[1]]={\"text\":[],\"true\":[],\"score\":[],\"binary_labels\":[],\"idx\":[],\"label\":[],\"datastplit\":[]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "5b550188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open AI\n",
    "\n",
    "for x in scores_openai:#[:1]:\n",
    "    p = json.load(open(asahi_scores_openai+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    if not dataset in results_score:\n",
    "        results_score[dataset]={\"metaphor\":{},\"anomaly\":{},\"literal\":{}}\n",
    "    for z in dlab:\n",
    "        if not model in results_score[dataset][z]:    \n",
    "            results_score[dataset][z][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "        for i,y in enumerate(p[z]):\n",
    "            results_score[dataset][z][model][\"score\"].append(y[\"score\"])\n",
    "            results_score[dataset][z][model][\"idx\"].append(y[\"index\"])\n",
    "            results_score[dataset][z][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "            assert results_score[dataset][z][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36f87dab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('baker', 'cake', 'chef', 'meal')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_score['Quadruples_Green_set'][\"metaphor\"][\"davinci\"][\"text\"][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "97a0e28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_score_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(results_score,outfile,indent=\"\\t\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abdf6ea",
   "metadata": {},
   "source": [
    "### No prompt  - all datasets but SAT_MET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6fcebc7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No prompt\n",
    "\n",
    "\n",
    "results_no_prompt = {}\n",
    "\n",
    "for x in no_prompt:#[:1]:\n",
    "    p = json.load(open(asahi_no_prompt+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    if not dataset in results_no_prompt:\n",
    "        results_no_prompt[dataset]={}\n",
    "    if not model in results_no_prompt[dataset]:    \n",
    "        results_no_prompt[dataset][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "    for i,y in enumerate(p[\"perplexity\"]):\n",
    "        results_no_prompt[dataset][model][\"score\"].append(y[\"score\"])\n",
    "        results_no_prompt[dataset][model][\"idx\"].append(y[\"index\"])\n",
    "        results_no_prompt[dataset][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "        assert results_no_prompt[dataset][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6efaaef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in no_prompt_openai:#[:1]:\n",
    "    p = json.load(open(asahi_no_prompt_openai+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    if not dataset in results_no_prompt:\n",
    "        results_no_prompt[dataset]={}\n",
    "    if not model in results_no_prompt[dataset]:    \n",
    "        results_no_prompt[dataset][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "    for i,y in enumerate(p[\"perplexity\"]):\n",
    "        results_no_prompt[dataset][model][\"score\"].append(y[\"score\"])\n",
    "        results_no_prompt[dataset][model][\"idx\"].append(y[\"index\"])\n",
    "        results_no_prompt[dataset][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "        assert results_no_prompt[dataset][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "95d566fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_no_prompt_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(results_no_prompt,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95e0d42",
   "metadata": {},
   "source": [
    "### Instruction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8cce6dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instruction : all questions\n",
    "\n",
    "# Open source\n",
    "\n",
    "results_instructions ={} \n",
    "\n",
    "for x in instructions:#[:1]:\n",
    "    p = json.load(open(asahi_instructions+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    if not dataset in results_instructions:\n",
    "        results_instructions[dataset]={\"metaphor\":{},\"anomaly\":{},\"literal\":{}}\n",
    "    for z in dlab:\n",
    "        if not model in results_instructions[dataset][z]:    \n",
    "            results_instructions[dataset][z][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "        for i,y in enumerate(p[z]):\n",
    "            results_instructions[dataset][z][model][\"score\"].append(y[\"score\"])\n",
    "            results_instructions[dataset][z][model][\"idx\"].append(y[\"index\"])\n",
    "            results_instructions[dataset][z][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "            assert results_instructions[dataset][z][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4d4bce0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open source\n",
    "\n",
    "\n",
    "for x in instructions_openai:#[:1]:\n",
    "    p = json.load(open(asahi_openai_instructions+\"/\"+x))\n",
    "    x = x.replace(\".Metaphors_and_Analogies_\",\"|\")\n",
    "    w = x.split(\"|\")\n",
    "    model,dataset=w[0],w[1][:-10]\n",
    "    if not dataset in results_instructions:\n",
    "        results_instructions[dataset]={\"metaphor\":{},\"anomaly\":{},\"literal\":{}}\n",
    "    for z in dlab:\n",
    "        if not model in results_instructions[dataset][z]:    \n",
    "            results_instructions[dataset][z][model] = {\"text\":[],\"true\":Flatten(p[\"labels\"]),\"idx\":[],\"score\":[]}\n",
    "        for i,y in enumerate(p[z]):\n",
    "            results_instructions[dataset][z][model][\"score\"].append(y[\"score\"])\n",
    "            results_instructions[dataset][z][model][\"idx\"].append(y[\"index\"])\n",
    "            results_instructions[dataset][z][model][\"text\"].append(datasets_idx2sent[dataset][i])\n",
    "            assert results_instructions[dataset][z][model][\"true\"][i]==datasets_sent2label[dataset][datasets_idx2sent[dataset][i]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "edf2b6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_instructions_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(results_instructions,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f31fbb",
   "metadata": {},
   "source": [
    "## SAT_MET Dictionaries \n",
    "- satsetid : sat quadruples to set_ids\n",
    "- satlabs : quadruples to labels\n",
    "- satplit : quadruples to data_split, randomm and lexical\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5efefc38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satlabs = {}\n",
    "for x in all_data[\"Quadruples_SAT_MET_FILTERED_set\"]['test']:\n",
    "    for i,pair in enumerate(x[\"pairs\"]):\n",
    "        target = eval(x[\"stem\"])\n",
    "        target.extend(pair)\n",
    "        satlabs[tuple(target)]=x[\"labels\"][i]\n",
    "        \n",
    "len(satlabs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "768405b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1710"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "satsetid = {}\n",
    "for x in all_data[\"Quadruples_SAT_MET_FILTERED_set\"]['test']:\n",
    "    for i,pair in enumerate(x[\"pairs\"]):\n",
    "        target = eval(x[\"stem\"])\n",
    "        target.extend(pair)\n",
    "        satsetid[tuple(target)]=x[\"id\"]\n",
    "        #print(x[\"pair_ids\"])\n",
    "        \n",
    "len(satsetid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "72129c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "satsplits = {\"lexical\":{},\"random\":{}}\n",
    "for z in dsplits:\n",
    "    for x in all_data[\"Quadruples_SAT_MET_FILTERED_lexical_split\"][z]:\n",
    "        r = x[\"AB\"]\n",
    "        r.extend(x[\"CD\"])\n",
    "        satsplits[\"lexical\"][tuple(r)]=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "c0130986",
   "metadata": {},
   "outputs": [],
   "source": [
    "for z in dsplits:\n",
    "    for x in all_data[\"Quadruples_SAT_MET_FILTERED_random_split\"][z]:\n",
    "        r = x[\"AB\"]\n",
    "        r.extend(x[\"CD\"])\n",
    "        satsplits[\"random\"][tuple(r)]=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c8c46816",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_sat_score = {\"metaphor\":{},\"anomaly\":{},\"analogy\":{}}\n",
    "\n",
    "for x  in asahi_sat_score_met:\n",
    "    results_sat_score[\"metaphor\"][x[1]]={\"text\":[],\"true\":[],\"score\":[],\"binary_labels\":[],\"idx\":[],\"label\":[]}\n",
    "    p = json.load(open(asahi_sat_score +x[0]))\n",
    "    for i, y in enumerate(p):\n",
    "        if tuple(y[\"target\"]) in satlabs:\n",
    "            results_sat_score[\"metaphor\"][x[1]][\"text\"].append(y[\"target\"])\n",
    "            results_sat_score[\"metaphor\"][x[1]][\"binary_labels\"].append(y['label'])\n",
    "            results_sat_score[\"metaphor\"][x[1]][\"score\"].append(y[\"ppl\"])\n",
    "            results_sat_score[\"metaphor\"][x[1]][\"true\"].append(satlabs[tuple(y[\"target\"])])\n",
    "            results_sat_score[\"metaphor\"][x[1]][\"idx\"].append(y[\"index\"])\n",
    "            \n",
    "for x  in asahi_sat_score_lit:\n",
    "    results_sat_score[\"analogy\"][x[1]]={\"text\":[],\"true\":[],\"score\":[],\"binary_labels\":[],\"idx\":[],\"label\":[]}\n",
    "    p = json.load(open(asahi_sat_score +x[0]))\n",
    "    for i, y in enumerate(p):\n",
    "        if tuple(y[\"target\"]) in satlabs:\n",
    "            results_sat_score[\"analogy\"][x[1]][\"text\"].append(y[\"target\"])\n",
    "            results_sat_score[\"analogy\"][x[1]][\"binary_labels\"].append(y['label'])\n",
    "            results_sat_score[\"analogy\"][x[1]][\"score\"].append(y[\"ppl\"])\n",
    "            results_sat_score[\"analogy\"][x[1]][\"true\"].append(satlabs[tuple(y[\"target\"])])\n",
    "            results_sat_score[\"analogy\"][x[1]][\"idx\"].append(y[\"index\"])\n",
    "            \n",
    "            \n",
    "for x  in asahi_sat_score_ano:\n",
    "    results_sat_score[\"anomaly\"][x[1]]={\"text\":[],\"true\":[],\"score\":[],\"binary_labels\":[],\"idx\":[],\"label\":[]}\n",
    "    p = json.load(open(asahi_sat_score +x[0]))\n",
    "    for i, y in enumerate(p):\n",
    "        if tuple(y[\"target\"]) in satlabs:\n",
    "            results_sat_score[\"anomaly\"][x[1]][\"text\"].append(y[\"target\"])\n",
    "            results_sat_score[\"anomaly\"][x[1]][\"binary_labels\"].append(y['label'])\n",
    "            results_sat_score[\"anomaly\"][x[1]][\"score\"].append(y[\"ppl\"])\n",
    "            results_sat_score[\"anomaly\"][x[1]][\"true\"].append(satlabs[tuple(y[\"target\"])])\n",
    "            results_sat_score[\"anomaly\"][x[1]][\"idx\"].append(y[\"index\"])\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "a04a7bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_sat_score_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(results_sat_score,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63abcd56",
   "metadata": {},
   "source": [
    "### SAT_met no score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "017b4399",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dir = \"asahi_results/sat-scores-no-prompt-other-models/\"\n",
    "a = os.listdir(res_dir)\n",
    "a = [x for x in a if \"sat_full_None.prompt\" in x]\n",
    "didisat = {}\n",
    "#print(sattext)\n",
    "sat_no_score = {}\n",
    "for x in a:\n",
    "    p = json.load(open(res_dir+x))\n",
    "    model = (x[:-26])\n",
    "    sat_no_score[model]={\"text\":[],\"score\":[],\"true\":[],\"idx\":[]}\n",
    "    for i,y in enumerate(p):\n",
    "        q = {}\n",
    "        text = y[\"input\"]+y[\"output\"]\n",
    "        flag = 0\n",
    "        for quad in satlabs:\n",
    "            c = 0\n",
    "            for word in quad:\n",
    "                if word in text :\n",
    "                    c+=1\n",
    "            if c==4:\n",
    "                mes4mots = quad\n",
    "                flag = 1\n",
    "        if flag==1:\n",
    "            sat_no_score[model][\"score\"].append(y[\"score\"])\n",
    "            sat_no_score[model][\"text\"].append(mes4mots)\n",
    "            sat_no_score[model][\"true\"].append(satlabs[mes4mots])\n",
    "            sat_no_score[model][\"idx\"].append(satsetid [mes4mots])\n",
    "            didisat[i]=mes4mots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "9881927c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = json.load(open(\"asahi_results/gpt3-scores-noprompt-SAT/davinci_sat_full_None.prompt.json\"))\n",
    "sat_no_score[\"davinci\"] = {\"text\":sat_no_score[model][\"text\"],\"score\":[],\"true\":sat_no_score[model][\"true\"],\"idx\":sat_no_score[model][\"idx\"]}\n",
    "n = 0\n",
    "for x in p:\n",
    "    for y in x[1]:\n",
    "        if n in didisat:\n",
    "            sat_no_score[\"davinci\"][\"score\"].append(y)\n",
    "        n+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e7c64656",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_sat_no_prompt_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(sat_no_score,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cf11d9",
   "metadata": {},
   "source": [
    "## GPT4 and 3.5 ANSWERS  on SAT original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0e8de150-c14f-416e-a5be-03b8f6067fe7",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can only concatenate str (not \"list\") to str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69101/4107394961.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can only concatenate str (not \"list\") to str"
     ]
    }
   ],
   "source": [
    "p = json.load(open(res_dir+r))\n",
    "p[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ce4035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "res_dir = \"asahi_results/generation-sat-original/\"\n",
    "resfiles = os.listdir(res_dir)\n",
    "sat_openAI = {}\n",
    "\n",
    "for r in resfiles:\n",
    "    model = r.split(\".\")[0]\n",
    "    if \"full.1\" in r:\n",
    "        prompt = \"prompt1\"\n",
    "    else:\n",
    "        prompt=\"prompt3\"\n",
    "    sat_openAI[prompt+\"_\"+model] = {\"predicted\":[],\"idx\":[],\"true\":[]}\n",
    "    p = json.load(open(res_dir+r))\n",
    "    accuracy =[]\n",
    "    print(model)\n",
    "    for i,x in enumerate(p):\n",
    "        pred = -1\n",
    "        if \"Llama\" in model or \"Mix\" in model:\n",
    "            reply = x[\"reply\"][len(x[\"input\"]):].strip().strip(\":\").strip()\n",
    "            reply = reply.split(\"\\n\")[0]\n",
    "        else:\n",
    "            reply = x[\"reply\"]\n",
    "        out = x['input'].split(\"\\n\")[1:-1]\n",
    "        n = 0\n",
    "        if len(reply)==0:\n",
    "            n+=1\n",
    "        elif reply[0] in ['1', '2', '3', '4', '5']:\n",
    "            pred = int(reply[0]) - 1\n",
    "        elif reply[1] in ['1', '2', '3', '4', '5']:\n",
    "            pred = int(reply[1]) - 1\n",
    "        elif reply.replace(\"option \", \"\").replace(\"Option \", \"\").replace(\": \", \"\")[0] in ['1', '2', '3', '4', '5']:\n",
    "            pred = int(reply.replace(\"option \", \"\").replace(\"Option \", \"\").replace(\": \", \"\")[0]) - 1\n",
    "        elif any(reply[:-1].lower() in o.lower() for o in out):\n",
    "            pred = int([o for o in out if reply[:-1].lower() in o.lower()][0][0]) - 1\n",
    "        elif reply[5] in ['1', '2', '3', '4', '5']:\n",
    "            pred = int(reply[5]) - 1\n",
    "        else:\n",
    "            print()\n",
    "            #print(out)\n",
    "            print(\"|\"+reply+\"|\")\n",
    "            print()\n",
    "            #raise ValueError(\"unknown reply\")\n",
    "        accuracy.append(int(int(x['answer']) == pred))           \n",
    "        sat_openAI[prompt+\"_\"+model][\"predicted\"].append(pred)\n",
    "        sat_openAI[prompt+\"_\"+model][\"true\"].append(x[\"answer\"])\n",
    "        sat_openAI[prompt+\"_\"+model][\"idx\"].append(i)\n",
    "    print(prompt, model, mean(accuracy),\"empty answer\",n,\"n_parsed\",len(sat_openAI[prompt+\"_\"+model][\"predicted\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sat_openAI:\n",
    "    sat_openAI[x][\"class_metrics\"]=EvaluationMetrics(sat_openAI[x][\"predicted\"],sat_openAI[x][\"true\"])\n",
    "\n",
    "    \n",
    "for x in sat_openAI:\n",
    "    print(x)\n",
    "    print(\"\\t\\t\\t\\t\\t\", sat_openAI[x][\"class_metrics\"][\"accuracy\"],sat_openAI[x][\"class_metrics\"][\"macro-f1\"],sat_openAI[x][\"class_metrics\"][\"class_prf1\"][2])   \n",
    "   \n",
    "\n",
    "        \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_original_sat_openAI_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(sat_openAI,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de822e",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"Dataset\",\"model\",\"size\", \"Acc.\",\"Macro-F1\"]  \n",
    "lines = []\n",
    "for x in sat_openAI:\n",
    "    line = [\n",
    "        x,\n",
    "        \"-\",\n",
    "        \"-\",\n",
    "        sat_openAI[x][\"class_metrics\"][\"accuracy\"][\"standard\"],\n",
    "        sat_openAI[x][\"class_metrics\"][\"macro-f1\"],\n",
    "    ]\n",
    "    print(\"\\t\".join([str(x) for x in line]))\n",
    "    lines.append(line)\n",
    "        \n",
    "\n",
    "filename = \"original_SAT_zeroshot_set.csv\"\n",
    "df = pd.DataFrame(lines,columns=head)\n",
    "df.to_csv(filename, index=False)     \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a36d82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bd14b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "head = [\"hf_index\",\"prediction\",\"true\",\"set_id\"]\n",
    "for x in sat_openAI:\n",
    "    #for y in results_e3[x]:\n",
    "    lines = []\n",
    "    filename = \"error_analysis/GPT3.5&4_zeroshot_set_\"+x+\".csv\"\n",
    "    for i,e in enumerate(sat_openAI[x][\"true\"]): \n",
    "        if sat_openAI[x][\"true\"][i]!=sat_openAI[x][\"predicted\"][i]:\n",
    "            line = [\n",
    "                i,\n",
    "                sat_openAI[x][\"predicted\"][i],\n",
    "                sat_openAI[x][\"true\"][i],\n",
    "                sat_openAI[x][\"idx\"][i]\n",
    "            ]\n",
    "            lines.append(line)\n",
    "    df = pd.DataFrame(lines,columns=head)\n",
    "    df.to_csv(filename, index=False)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86674f82",
   "metadata": {},
   "source": [
    "## GPT 3.5 other sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f5f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see Asahi chat process script"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8101478",
   "metadata": {},
   "source": [
    "# Sat_met scores results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70d21c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_sat_combined = {}\n",
    "\n",
    "for m in results_sat_score[\"anomaly\"]:\n",
    "    three_sat_combined[m]={\"scores\":[],\"scaled_scores\":[],\"labels\":[],\"metrics\":[]}\n",
    "    max_ano = max(results_sat_score[\"anomaly\"][m][\"score\"])\n",
    "    min_ano = min(results_sat_score[\"anomaly\"][m][\"score\"])\n",
    "    max_ana = max(results_sat_score[\"analogy\"][m][\"score\"])\n",
    "    min_ana = min(results_sat_score[\"analogy\"][m][\"score\"])\n",
    "    max_met = max(results_sat_score[\"metaphor\"][m][\"score\"])\n",
    "    min_met = min(results_sat_score[\"metaphor\"][m][\"score\"])\n",
    "    for i in range(len(results_sat_score[\"anomaly\"][m][\"text\"])):   \n",
    "        ano = results_sat_score[\"anomaly\"][m][\"score\"][i]\n",
    "        scaled_ano = ((ano-min_ano)/(max_ano - min_ano))*0.75\n",
    "        ana = results_sat_score[\"analogy\"][m][\"score\"][i]\n",
    "        scaled_ana = ((ana-min_ana)/(max_ana - min_ana))\n",
    "        met = results_sat_score[\"metaphor\"][m][\"score\"][i]\n",
    "        scaled_met = ((met - min_met)/(max_met - min_met))\n",
    "        k = [ano,ana,met]\n",
    "        scaled_k = [scaled_ano,scaled_ana,scaled_met]\n",
    "        three_sat_combined[m][\"scores\"].append(k)\n",
    "        three_sat_combined[m][\"scaled_scores\"].append(scaled_k)\n",
    "        label = scaled_k.index(min(scaled_k))\n",
    "        #label = scaled_k.index(min(scaled_k))\n",
    "        three_sat_combined[m][\"labels\"].append(label)\n",
    "        \n",
    "sat_true = results_sat_score[\"metaphor\"][m][\"true\"]\n",
    "\n",
    "\n",
    "for m in three_sat_combined:\n",
    "    three_sat_combined[m][\"metrics\"]= EvaluationMetrics(three_sat_combined[m][\"labels\"],sat_true)      \n",
    "    print(m,'\\t',three_sat_combined[m][\"metrics\"][\"macro-f1\"],'\\t',three_sat_combined[m][\"metrics\"][\"class_prf1\"][2])\n",
    "    \n",
    "\n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_score3satcombines_complete_set_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(three_sat_combined,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d741829b",
   "metadata": {},
   "source": [
    "## SAT_Met results Random split test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc4b9a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_sat_combined = {}\n",
    "\n",
    "test_sc_ana,test_sc_met,test_sc_ano = [],[],[]\n",
    "test_sat_true = [ ]\n",
    "\n",
    "for i,s in enumerate(results_sat_score[\"analogy\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"analogy\"][m][\"text\"][i])\n",
    "    if satsplits[\"random\"][t]==\"test\":\n",
    "        test_sc_ana.append(s)\n",
    "        test_sat_true.append(results_sat_score[\"analogy\"][m][\"true\"][i])\n",
    "    \n",
    "for i,s in enumerate(results_sat_score[\"metaphor\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"metaphor\"][m][\"text\"][i])\n",
    "    if satsplits[\"random\"][t]==\"test\":\n",
    "        test_sc_met.append(s)\n",
    "\n",
    "for i,s in enumerate(results_sat_score[\"anomaly\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"anomaly\"][m][\"text\"][i])\n",
    "    if satsplits[\"random\"][t]==\"test\":\n",
    "        test_sc_ano.append(s)\n",
    "\n",
    "\n",
    "for m in results_sat_score[\"anomaly\"]:\n",
    "    three_sat_combined[m]={\"scores\":[],\"scaled_scores\":[],\"labels\":[],\"metrics\":[]}\n",
    "    max_ano = max(test_sc_ano)\n",
    "    min_ano = min(test_sc_ano)\n",
    "    max_ana = max(test_sc_ana)\n",
    "    min_ana = min(test_sc_ana)\n",
    "    max_met = max(test_sc_met)\n",
    "    min_met = min(test_sc_met)\n",
    "    for i in range(len(results_sat_score[\"anomaly\"][m][\"text\"])):  \n",
    "        t = tuple(results_sat_score[\"anomaly\"][m][\"text\"][i])\n",
    "        if satsplits[\"random\"][t]==\"test\":\n",
    "            ano = results_sat_score[\"anomaly\"][m][\"score\"][i]\n",
    "            scaled_ano = ((ano-min_ano)/(max_ano - min_ano))* 0.75\n",
    "            ana = results_sat_score[\"analogy\"][m][\"score\"][i]\n",
    "            scaled_ana = ((ana-min_ana)/(max_ana - min_ana))\n",
    "            met = results_sat_score[\"metaphor\"][m][\"score\"][i]\n",
    "            scaled_met = ((met - min_met)/(max_met - min_met))\n",
    "            k = [ano,ana,met]\n",
    "            scaled_k = [scaled_ano,scaled_ana,scaled_met]\n",
    "            three_sat_combined[m][\"scores\"].append(k)\n",
    "            three_sat_combined[m][\"scaled_scores\"].append(scaled_k)\n",
    "            label = scaled_k.index(min(scaled_k))\n",
    "            three_sat_combined[m][\"labels\"].append(label)\n",
    "        \n",
    "sat_true = results_sat_score[\"metaphor\"][m][\"true\"]\n",
    "\n",
    "\n",
    "for m in three_sat_combined:\n",
    "    three_sat_combined[m][\"metrics\"]= EvaluationMetrics(three_sat_combined[m][\"labels\"],test_sat_true)      \n",
    "    print(m,'\\t',three_sat_combined[m][\"metrics\"][\"macro-f1\"],'\\t',three_sat_combined[m][\"metrics\"][\"class_prf1\"][2])\n",
    "    \n",
    "\n",
    "    \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_score3satcombines_randomSPlit_testSet_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(three_sat_combined,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79422a6a",
   "metadata": {},
   "source": [
    "## SAT_Met results Lexical split test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d128e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "three_sat_combined = {}\n",
    "\n",
    "\n",
    "test_sc_ana,test_sc_met,test_sc_ano = [],[],[]\n",
    "test_sat_true = [ ]\n",
    "\n",
    "for i,s in enumerate(results_sat_score[\"analogy\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"analogy\"][m][\"text\"][i])\n",
    "    if satsplits[\"lexical\"][t]==\"test\":\n",
    "        test_sc_ana.append(s)\n",
    "        test_sat_true.append(results_sat_score[\"analogy\"][m][\"true\"][i])\n",
    "    \n",
    "for i,s in enumerate(results_sat_score[\"metaphor\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"metaphor\"][m][\"text\"][i])\n",
    "    if satsplits[\"lexical\"][t]==\"test\":\n",
    "        test_sc_met.append(s)\n",
    "\n",
    "for i,s in enumerate(results_sat_score[\"anomaly\"][m][\"score\"]):\n",
    "    t= tuple(results_sat_score[\"anomaly\"][m][\"text\"][i])\n",
    "    if satsplits[\"lexical\"][t]==\"test\":\n",
    "        test_sc_ano.append(s)\n",
    "\n",
    "\n",
    "for m in results_sat_score[\"anomaly\"]:\n",
    "    three_sat_combined[m]={\"scores\":[],\"scaled_scores\":[],\"labels\":[],\"metrics\":[]}\n",
    "    max_ano = max(test_sc_ano)\n",
    "    min_ano = min(test_sc_ano)\n",
    "    max_ana = max(test_sc_ana)\n",
    "    min_ana = min(test_sc_ana)\n",
    "    max_met = max(test_sc_met)\n",
    "    min_met = min(test_sc_met)\n",
    "    for i in range(len(results_sat_score[\"anomaly\"][m][\"text\"])):  \n",
    "        t = tuple(results_sat_score[\"anomaly\"][m][\"text\"][i])\n",
    "        if satsplits[\"lexical\"][t]==\"test\":\n",
    "            ano = results_sat_score[\"anomaly\"][m][\"score\"][i]\n",
    "            scaled_ano = ((ano-min_ano)/(max_ano - min_ano))*0.75\n",
    "            ana = results_sat_score[\"analogy\"][m][\"score\"][i]\n",
    "            scaled_ana = ((ana-min_ana)/(max_ana - min_ana))\n",
    "            met = results_sat_score[\"metaphor\"][m][\"score\"][i]\n",
    "            scaled_met = ((met - min_met)/(max_met - min_met))\n",
    "            k = [ano,ana,met]\n",
    "            scaled_k = [scaled_ano,scaled_ana,scaled_met]\n",
    "            three_sat_combined[m][\"scores\"].append(k)\n",
    "            three_sat_combined[m][\"scaled_scores\"].append(scaled_k)\n",
    "            label = scaled_k.index(min(scaled_k))\n",
    "            three_sat_combined[m][\"labels\"].append(label)\n",
    "        \n",
    "sat_true = results_sat_score[\"metaphor\"][m][\"true\"]\n",
    "\n",
    "\n",
    "for m in three_sat_combined:\n",
    "    three_sat_combined[m][\"metrics\"]= EvaluationMetrics(three_sat_combined[m][\"labels\"],test_sat_true)      \n",
    "    print(m,'\\t',three_sat_combined[m][\"metrics\"][\"macro-f1\"],'\\t',three_sat_combined[m][\"metrics\"][\"class_prf1\"][2])\n",
    "   \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_score3satcombines_lexicalSplit_testSet_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(three_sat_combined,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3056d5",
   "metadata": {},
   "source": [
    "# SAT_MET score on our labels  binary met-analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93238296",
   "metadata": {},
   "outputs": [],
   "source": [
    "two_sc_ana,two_sc_met = [],[]\n",
    "for i,s in enumerate(results_sat_score[\"analogy\"][m][\"score\"]):\n",
    "    if results_sat_score[\"analogy\"][m][\"true\"][i]!=0:\n",
    "        two_sc_ana.append(s)\n",
    "    \n",
    "for i,s in enumerate(results_sat_score[\"metaphor\"][m][\"score\"]):\n",
    "    if results_sat_score[\"metaphor\"][m][\"true\"][i]!=0:\n",
    "        two_sc_met.append(s)\n",
    "\n",
    "print(len(two_sc_ana))\n",
    "    \n",
    "two_sat_combined = {}\n",
    "for m in results_sat_score[\"anomaly\"]:\n",
    "    two_sat_combined[m]={\"scores\":[],\"scaled_scores\":[],\"labels\":[],\"metrics\":[]}\n",
    "    max_ana = max(two_sc_ana)\n",
    "    min_ana = min(two_sc_ana)\n",
    "    max_met = max(two_sc_met)\n",
    "    min_met = min(two_sc_met)\n",
    "    for i in range(len(results_sat_score[\"anomaly\"][m][\"text\"])): \n",
    "        if sat_true[i]!=0:\n",
    "            ana = results_sat_score[\"analogy\"][m][\"score\"][i]\n",
    "            scaled_ana = ((ana-min_ana)/(max_ana - min_ana))\n",
    "            met = results_sat_score[\"metaphor\"][m][\"score\"][i]\n",
    "            scaled_met = ((met - min_met)/(max_met - min_met))\n",
    "            k = [ana,met]\n",
    "            scaled_k = [scaled_ana,scaled_met]\n",
    "            two_sat_combined[m][\"scores\"].append(k)\n",
    "            two_sat_combined[m][\"scaled_scores\"].append(scaled_k)\n",
    "            #label = k.index(min(k))\n",
    "            label = scaled_k.index(min(scaled_k))\n",
    "            two_sat_combined[m][\"labels\"].append(label+1)\n",
    "        \n",
    "sat_true = results_sat_score[\"metaphor\"][m][\"true\"]\n",
    "two_sat_true = [x for x in sat_true if x!=0]\n",
    "\n",
    "for m in two_sat_combined:\n",
    "    two_sat_combined[m][\"metrics\"]= EvaluationMetrics(two_sat_combined[m][\"labels\"],two_sat_true)      \n",
    "    #print(three_sat_combined[m][\"labels\"][:10])\n",
    "    print(m,'\\t','\\t',two_sat_combined[m][\"metrics\"][\"macro-f1\"],'\\t',two_sat_combined[m][\"metrics\"][\"class_prf1\"][2])\n",
    "    \n",
    "    \n",
    "output = \"utils_dictionaries/\"\n",
    "with open(output+\"results_score2satcombined_binary_complete_set_zero_shot.json\", \"w\") as outfile:\n",
    "    json.dump(two_sat_combined,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1ef36c",
   "metadata": {},
   "source": [
    "# Other datasets : score, zero shot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d54d865",
   "metadata": {},
   "source": [
    "## GPT 3's perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad01f1d",
   "metadata": {},
   "source": [
    "## GPT4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ec0e99-352e-4b5c-b3ad-cb439d57ba39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "87c9d3e7",
   "metadata": {},
   "source": [
    "# Output useful dictionaries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7752da",
   "metadata": {},
   "source": [
    "# Joanne results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4185c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "output = \"utils_dictionaries/\"\n",
    "\n",
    "datasets_sent2labelk = {}\n",
    "datasets_sent2idxk = {}\n",
    "datasets_datasplitsk = {}\n",
    "\n",
    "\n",
    "for dataset in datasets_sent2label:\n",
    "    datasets_sent2labelk[dataset] = {}\n",
    "    for x in datasets_sent2label[dataset]:\n",
    "        datasets_sent2labelk[dataset][str(x)]=datasets_sent2label[dataset][x]\n",
    "        \n",
    "for dataset in datasets_sent2idx:\n",
    "    datasets_sent2idxk[dataset] = {}\n",
    "    for x in datasets_sent2idx[dataset]:\n",
    "        datasets_sent2idxk[dataset][str(x)]=datasets_sent2idx[dataset][x]\n",
    "        \n",
    "for dataset in datasets_datasplits:\n",
    "    datasets_datasplitsk[dataset] = {\"random\":{},\"lexical\":{}}\n",
    "    for fpl in datasets_datasplits[dataset]:\n",
    "        for x in datasets_datasplits[dataset][fpl]:\n",
    "            datasets_datasplitsk[dataset][fpl][str(x)]=datasets_datasplits[dataset][fpl][x]\n",
    "        \n",
    "\n",
    "print(1)\n",
    "\n",
    "with open(output+\"datasets_sent2label.json\", \"w\") as outfile:\n",
    "    json.dump(datasets_sent2labelk,outfile,indent=\"\\t\")\n",
    "  \n",
    "print(2)\n",
    "\n",
    "with open(output+\"datasets_sent2idx.json\", \"w\") as outfile:\n",
    "    json.dump(datasets_sent2idxk,outfile,indent=\"\\t\")\n",
    "  \n",
    "print(3)\n",
    "\n",
    "with open(output+\"datasets_idx2sent.json\", \"w\") as outfile:\n",
    "    json.dump(datasets_idx2sent,outfile,indent=\"\\t\")\n",
    "\n",
    "print(4)\n",
    "\n",
    "with open(output+\"datasets_datasplits.json\", \"w\") as outfile:\n",
    "    json.dump(datasets_datasplitsk,outfile,indent=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cbdf4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joanne_t5_finetuning = \"joanne_results\"\n",
    "joanne_t5_finetuning = \"joanne_results\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b68b1d2",
   "metadata": {},
   "source": [
    "## Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "732ff494",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_69101/2586536081.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;34m\"Cardillo\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mset_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmylabels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Cardillo\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'results' is not defined"
     ]
    }
   ],
   "source": [
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for data in results:\n",
    "  if \"Cardillo\" in data:\n",
    "    set_labels = mylabels[\"Cardillo\"] \n",
    "  elif \"Pairs\" in data:\n",
    "    set_labels = mylabels[\"Pairs\"] \n",
    "  elif \"Quadruples\" in data:\n",
    "    set_labels = mylabels[\"Quadruples\"]\n",
    "  print(metrics.classification_report(results[data][\"true\"], results[data][\"generated\"]))\n",
    "  cm = metrics.confusion_matrix(results[data][\"true\"], results[data][\"generated\"])\n",
    "  df_cm = pd.DataFrame(cm, index = set_labels, columns = set_labels)\n",
    "  plt.figure(figsize = (10,7))\n",
    "  sn.heatmap(df_cm, annot=True, cmap='Purples', fmt='g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a04206-b857-47fe-bfdb-61f689f24df1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
